{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üß† What This Notebook Does\n",
        "ü™û Mirror System Operational Chatbot\n",
        "You‚Äôre running an interactive, agentic GPT-4o chatbot with memory, task awareness, and investigative backend logic ‚Äî right inside Colab.\n",
        "\n",
        "‚úÖ Core Capabilities\n",
        "1. Interactive GPT-4o Chat\n",
        "You can chat in natural language with GPT-4o.\n",
        "\n",
        "The assistant remembers conversation history (via conversation_history).\n",
        "\n",
        "Chat ends cleanly when you type exit, quit, or q.\n",
        "\n",
        "2. STRYXX-1 Construct: Agentic Task Memory\n",
        "Tracks implicit tasks based on your conversation (extract_and_rank_tasks()).\n",
        "\n",
        "Commands you can use:\n",
        "\n",
        "/stack ‚Üí shows your active task stack.\n",
        "\n",
        "/last ‚Üí shows your last completed task.\n",
        "\n",
        "/next ‚Üí shows GPT‚Äôs suggestion for your next best action.\n",
        "\n",
        "You can also trigger STRYXX in manual override mode for things like:\n",
        "\n",
        "\"STRYXX-1: priority map\" ‚Üí prints current mission ranking.\n",
        "\n",
        "\"STRYXX-1: task audit\" ‚Üí checks redundancy.\n",
        "\n",
        "These live in memory as:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "stryxx_memory = {\n",
        "    \"task_stack\": [...],\n",
        "    \"last_task\": \"...\",\n",
        "    \"next_suggestion\": \"...\"\n",
        "}\n",
        "3. BASTION-9 Construct (optional filter)\n",
        "(If enabled later) locks responses to factual zones and filters hallucinations.\n",
        "\n",
        "Currently optional ‚Äî logic can be added for:\n",
        "\n",
        "sandbox filtering\n",
        "\n",
        "active enforcement\n",
        "\n",
        "hallucination detection\n",
        "\n",
        "4. BigQuery Access\n",
        "Two clients are set up:\n",
        "\n",
        "eleven-team-safety\n",
        "\n",
        "xi-labs\n",
        "\n",
        "You can:\n",
        "\n",
        "Query real production datasets directly.\n",
        "\n",
        "Wire natural language to SQL via a future STRYXX extension.\n",
        "\n",
        "Investigate users, fingerprints, voice use, etc.\n",
        "\n",
        "üõ†Ô∏è Advanced Extensibility (Already Wired or Stubbed)\n",
        "Feature\tStatus\tNotes\n",
        "üîó OpenAI API\t‚úÖ\tUsing GPT-4o via LiteLLM\n",
        "üß† Conversation memory\t‚úÖ\tStored in conversation_history\n",
        "‚öî STRYXX task memory\t‚úÖ\tTask stack + suggestions\n",
        "üõ° BASTION-9 logic\tüü°\tFramework uploaded, not enforced yet\n",
        "üìä BigQuery dual-project support\t‚úÖ\televen-team-safety + xi-labs\n",
        "üí¨ STRYXX override commands\t‚úÖ\tVia keyword triggers\n",
        "üß© Notebook modular structure\t‚úÖ\tClean cells by purpose\n",
        "üóÇÔ∏è Summon Cards + Docs\t‚úÖ\tAll loaded + mapped\n",
        "\n",
        "üó£Ô∏è What You Can Ask Right Now\n",
        "You can use this chatbot for:\n",
        "\n",
        "ü§ñ Asking about anything GPT-4o supports (legal help, writing, debugging)\n",
        "\n",
        "‚öî Strategy planning ‚Äî STRYXX will extract and stack your goals\n",
        "\n",
        "üíª Investigations ‚Äî ask things like:\n",
        "\n",
        "\"Query top TTS users by fingerprint\"\n",
        "\n",
        "\"Check device types in xi-labs\"\n",
        "\n",
        "üß† Personal flow tracking ‚Äî STRYXX helps keep momentum\n",
        "\n",
        "üìä Future plans ‚Äî wire your GPT directly into abuse detection, TTS logs, and workspace analysis\n",
        "\n",
        "üí° Next You Could Add:\n",
        "BASTION-9 hallucination filtering per reply (auto or manual)\n",
        "\n",
        "A /query STRYXX command that auto-translates questions to SQL\n",
        "\n",
        "Memory persistence (save/load STRYXX + chat history)\n",
        "\n",
        "Slack or web UI endpoint for team use\n",
        "\n",
        "GPT-based summary at end of chat session"
      ],
      "metadata": {
        "id": "stV4pFdYlqvi"
      },
      "id": "stV4pFdYlqvi"
    },
    {
      "cell_type": "code",
      "source": [
        "last_response = None  # Used to store last GPT reply for download purposes\n"
      ],
      "metadata": {
        "id": "0DZPKgSKrXD-"
      },
      "id": "0DZPKgSKrXD-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "aptmnJM8yf0MkmiTgYLMRw51",
      "metadata": {
        "tags": [],
        "id": "aptmnJM8yf0MkmiTgYLMRw51"
      },
      "source": [
        "# @title Environment Setup (Agentic GPT + BigQuery Dual Project Support)\n",
        "\n",
        "# Install core libraries\n",
        "!pip install --quiet --upgrade \\\n",
        "    litellm \\\n",
        "    openai \\\n",
        "    tiktoken \\\n",
        "    google-cloud-bigquery \\\n",
        "    google-auth\n",
        "!pip install PyPDF2 --quiet\n",
        "#  Runtime Restart Required (automatically triggered below)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf python-docx --quiet"
      ],
      "metadata": {
        "id": "E7BsaP82qvuJ"
      },
      "id": "E7BsaP82qvuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîê OpenAI + LiteLLM Setup\n",
        "from getpass import getpass\n",
        "import openai\n",
        "from litellm import completion\n",
        "\n",
        "# Ask for your API key securely\n",
        "api_key = getpass(\"üîê Enter your OpenAI API key: \")\n",
        "\n",
        "# Set OpenAI key for use with openai lib (if needed)\n",
        "openai.api_key = api_key\n",
        "\n",
        "print(\"‚úÖ API Key stored for OpenAI + LiteLLM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "leREZp_qehTV",
        "outputId": "4d5b6934-26b9-4663-e957-e4a4781c82da"
      },
      "id": "leREZp_qehTV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîê Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ API Key stored for OpenAI + LiteLLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîÅ Load Session from Uploaded JSON\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))  # Take first uploaded file\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "conversation_history = data.get(\"conversation_history\", [])\n",
        "stryxx_memory = data.get(\"stryxx_memory\", {\n",
        "    \"task_stack\": [],\n",
        "    \"last_task\": None,\n",
        "    \"next_suggestion\": None\n",
        "})\n",
        "\n",
        "print(\"‚úÖ Session restored from file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "cellView": "form",
        "id": "smQjGWYpsPyo",
        "outputId": "aed1de79-be8b-497b-d792-a5d6edf8ac33"
      },
      "id": "smQjGWYpsPyo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1209aeeb-9df8-4f72-9c20-8154f3aa1d30\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1209aeeb-9df8-4f72-9c20-8154f3aa1d30\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2935017243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Take first uploaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß† STRYXX + BASTION Memory Initialization\n",
        "\n",
        "# Memory states for constructs\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Stay grounded, remember facts, and guide the user with clarity and focus.\"}\n",
        "]\n",
        "\n",
        "stryxx_memory = {\n",
        "    \"task_stack\": [],\n",
        "    \"last_task\": None,\n",
        "    \"next_suggestion\": None\n",
        "}\n",
        "\n",
        "bastion_memory = {\n",
        "    \"last_truth_check\": None,\n",
        "    \"last_warning\": None\n",
        "}\n",
        "\n",
        "constructs = {\n",
        "    \"STRYXX-1\": {\"active\": False},\n",
        "    \"BASTION-9\": {\"active\": False}\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X9CKCwoxfOQH"
      },
      "id": "X9CKCwoxfOQH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚öôÔ∏è Construct Command Parsers (STRYXX + BASTION)\n",
        "\n",
        "def handle_stryxx_commands(user_input):\n",
        "    command = user_input.strip().lower()\n",
        "\n",
        "    if command == \"/stack\":\n",
        "        return \"üóÇÔ∏è Task Stack:\\n\" + \"\\n\".join(\n",
        "            [f\"{i+1}. {t}\" for i, t in enumerate(stryxx_memory['task_stack'])]\n",
        "        ) or \"No tasks in stack.\"\n",
        "\n",
        "    elif command == \"/last\":\n",
        "        return f\"üïì Last completed task:\\n{stryxx_memory['last_task'] or 'None yet.'}\"\n",
        "\n",
        "    elif command == \"/next\":\n",
        "        return f\"üéØ Next suggested task:\\n{stryxx_memory['next_suggestion'] or 'None generated yet.'}\"\n",
        "\n",
        "    return None  # Not a command\n",
        "\n",
        "\n",
        "def handle_bastion_commands(user_input):\n",
        "    if user_input.strip().lower() == \"/truth\":\n",
        "        return f\"üõ°Ô∏è Last truth enforcement:\\n{bastion_memory['last_truth_check'] or 'None yet.'}\"\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_4sj9f-Iet9S"
      },
      "id": "_4sj9f-Iet9S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß† STRYXX Extract + Rank Tasks\n",
        "\n",
        "def extract_and_rank_tasks(conversation):\n",
        "    from operator import itemgetter\n",
        "\n",
        "    summary_prompt = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are STRYXX-1, an Execution Strategist.\\n\"\n",
        "            \"Your job is to extract the tasks the user is implicitly or explicitly working on, \"\n",
        "            \"rank them by ROI (return on impact + time), and suggest the next most useful action.\\n\"\n",
        "            \"Only include tasks that drive momentum or clarity. Discard fluff.\\n\\n\"\n",
        "            \"Output:\\n\"\n",
        "            \"- Ranked task stack (list of short task labels)\\n\"\n",
        "            \"- Most recent completed task (if any)\\n\"\n",
        "            \"- Next recommended step\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": \"\\n\".join([\n",
        "            f\"{msg['role']}: {msg['content']}\"\n",
        "            for msg in conversation if msg[\"role\"] in [\"user\", \"assistant\"]\n",
        "        ])}\n",
        "    ]\n",
        "\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=summary_prompt,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        lines = text.splitlines()\n",
        "        task_stack = []\n",
        "        last_task = None\n",
        "        next_suggestion = None\n",
        "        mode = None\n",
        "\n",
        "        for line in lines:\n",
        "            if line.strip().startswith(\"- Ranked task stack\"):\n",
        "                mode = \"stack\"\n",
        "                continue\n",
        "            elif line.strip().startswith(\"- Most recent completed task\"):\n",
        "                mode = \"last\"\n",
        "                continue\n",
        "            elif line.strip().startswith(\"- Next recommended step\"):\n",
        "                mode = \"next\"\n",
        "                continue\n",
        "\n",
        "            if mode == \"stack\" and line.strip().startswith(tuple(str(i) + \".\" for i in range(1, 10))):\n",
        "                task_stack.append(line.split(\".\", 1)[1].strip())\n",
        "            elif mode == \"last\":\n",
        "                last_task = line.strip(\"- \").strip()\n",
        "            elif mode == \"next\":\n",
        "                next_suggestion = line.strip(\"- \").strip()\n",
        "\n",
        "        # Update memory\n",
        "        stryxx_memory[\"task_stack\"] = task_stack\n",
        "        stryxx_memory[\"last_task\"] = last_task\n",
        "        stryxx_memory[\"next_suggestion\"] = next_suggestion\n",
        "\n",
        "        return \"‚úÖ STRYXX memory updated.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è STRYXX failed to parse response:\\n\\n{text}\\n\\nError: {e}\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IrEnUFfKfZx1"
      },
      "id": "IrEnUFfKfZx1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß† STRYXX Activation + Response Handler\n",
        "def activate_stryxx():\n",
        "    constructs[\"STRYXX-1\"][\"active\"] = True\n",
        "    return (\n",
        "        \"STRYXX-1 online.\\n\"\n",
        "        \"Please confirm desired next action:\\n\"\n",
        "        \"‚Ä¢ full priority map\\n‚Ä¢ task audit of current thread\\n‚Ä¢ or await further instruction\"\n",
        "    )\n",
        "\n",
        "def stryxx_response(user_input, thread=None):\n",
        "    if \"priority map\" in user_input.lower():\n",
        "        return \"**STRYXX-1 PRIORITY MAP**\\n1. Connect chatbot to BigQuery\\n2. Set up memory scaffolding\\n3. Implement user feedback capture\"\n",
        "    elif \"task audit\" in user_input.lower():\n",
        "        return \"**STRYXX-1 TASK AUDIT**\\nNo redundant tasks. Current focus valid.\"\n",
        "    else:\n",
        "        return (\n",
        "            \"STRYXX-1 awaiting operational input. Try commands like:\\n\"\n",
        "            \"‚Ä¢ STRYXX-1: PRIORITY MODE\\n‚Ä¢ STRYXX-1: override emotional pull\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "qG6F6k8Ufimh"
      },
      "id": "qG6F6k8Ufimh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def run_bigquery_query(query, default_project=\"xi-labs\"):\n",
        "    try:\n",
        "        # üîç Try to detect project from the query itself\n",
        "        match = re.search(r\"FROM\\s+`([\\w\\-]+)\\.\", query, re.IGNORECASE)\n",
        "        project = match.group(1) if match else default_project\n",
        "\n",
        "        client = bigquery.Client(project=project)\n",
        "        result = client.query(query).result().to_dataframe()\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå BigQuery Error: {e}\"\n"
      ],
      "metadata": {
        "id": "a3fzHdcbMlkS"
      },
      "id": "a3fzHdcbMlkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chat With Memory + STRYXX + BigQuery + File Tools\n",
        "# @title üí¨ Chat With Memory + STRYXX + NL BigQuery + File I/O + Auto-Save\n",
        "\n",
        "import time\n",
        "import threading\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output, display\n",
        "from litellm import completion\n",
        "\n",
        "conversation_history = [{\"role\": \"system\", \"content\": \"You are a GPT-4o assistant with full memory, file analysis, and BigQuery access. Always respond concisely and helpfully.\"}]\n",
        "last_response = None\n",
        "bq_progress_bar = None\n",
        "\n",
        "def run_query_with_progress(sql, project=\"xi-labs\"):\n",
        "    global bq_progress_bar\n",
        "\n",
        "    def progress():\n",
        "        for _ in tqdm(range(100), desc=\"üß† Executing BigQuery\", ncols=80):\n",
        "            time.sleep(0.03)\n",
        "\n",
        "    # Start progress bar\n",
        "    thread = threading.Thread(target=progress)\n",
        "    thread.start()\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "    client = bigquery.Client(project=project)\n",
        "    result = client.query(sql).to_dataframe()\n",
        "\n",
        "    thread.join()  # Wait for progress bar to finish\n",
        "    return result\n",
        "\n",
        "def convert_nl_to_sql(user_input):\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"Convert the user's request into a BigQuery SQL query. Use only tables from 'xi-labs' and 'eleven-team-safety'. Assume user has access.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "    response = completion(model=\"openai/gpt-4o\", messages=prompt, api_key=api_key)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def chat_with_memory(user_input, model=\"openai/gpt-4o\"):\n",
        "    global last_response\n",
        "\n",
        "    # STRYXX command check\n",
        "    stryxx_output = handle_stryxx_commands(user_input)\n",
        "    if stryxx_output:\n",
        "        return f\"üß† STRYXX: {stryxx_output}\"\n",
        "\n",
        "    # üóÇÔ∏è File analysis\n",
        "    for filename in uploaded_file_contents:\n",
        "        if filename.lower() in user_input.lower():\n",
        "            file_text = uploaded_file_contents[filename]\n",
        "            analysis_prompt = [\n",
        "                {\"role\": \"system\", \"content\": f\"You are a document analyst. File name: '{filename}'\"},\n",
        "                {\"role\": \"user\", \"content\": file_text[:3000]},\n",
        "                {\"role\": \"user\", \"content\": user_input}\n",
        "            ]\n",
        "            response = completion(model=model, messages=analysis_prompt, api_key=api_key)\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "    # üì• File download trigger\n",
        "    if \"download as\" in user_input.lower():\n",
        "        for filetype in [\"pdf\", \"csv\", \"docx\"]:\n",
        "            if filetype in user_input.lower():\n",
        "                return generate_and_download_file(filetype, last_response or \"No content to save.\")\n",
        "\n",
        "    # üìä Natural Language ‚Üí SQL ‚Üí Query\n",
        "    if \"query\" in user_input.lower() or \"get\" in user_input.lower():\n",
        "        try:\n",
        "            sql = convert_nl_to_sql(user_input)\n",
        "            # Decide which project\n",
        "            project = \"eleven-team-safety\" if \"eleven\" in sql else \"xi-labs\"\n",
        "            df = run_query_with_progress(sql, project=project)\n",
        "            return df.to_markdown()\n",
        "        except Exception as e:\n",
        "            return f\"‚ö†Ô∏è BigQuery failed:\\n{e}\"\n",
        "\n",
        "    # üí¨ Standard GPT chat\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    response = completion(model=model, messages=conversation_history, api_key=api_key)\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "    last_response = reply\n",
        "    return reply\n",
        "\n"
      ],
      "metadata": {
        "id": "PVjsMMrnfpHk",
        "cellView": "form"
      },
      "id": "PVjsMMrnfpHk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚öîÔ∏è STRYXX Activation + Manual Response Handler\n",
        "def activate_stryxx():\n",
        "    constructs[\"STRYXX-1\"][\"active\"] = True\n",
        "    return \"STRYXX-1 online. Please confirm desired next action: full priority map, task audit of current thread, or await further instruction?\"\n",
        "\n",
        "def stryxx_response(user_input, thread):\n",
        "    command = user_input.lower()\n",
        "\n",
        "    if \"priority map\" in command:\n",
        "        return \"**STRYXX-1 PRIORITY MAP**\\n1. Connect chatbot to BigQuery\\n2. Set up memory scaffolding\\n3. Implement user feedback capture\"\n",
        "    elif \"task audit\" in command:\n",
        "        return \"**STRYXX-1 TASK AUDIT**\\nNo redundant tasks. Current focus valid.\"\n",
        "    else:\n",
        "        return \"STRYXX-1 awaiting operational input. Use commands like:\\n‚Ä¢ STRYXX-1: PRIORITY MODE\\n‚Ä¢ STRYXX-1: override emotional pull\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iSNWVn4sfvvs"
      },
      "id": "iSNWVn4sfvvs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üïí Background Autosave (Every 3 minutes)\n",
        "import threading, time, datetime, json\n",
        "\n",
        "# üîß Custom settings\n",
        "AUTOSAVE_INTERVAL_MINUTES = 3\n",
        "AUTOSAVE_FOLDER = \"/content/autosaves\"  # ‚úÖ Change this to any writable path\n",
        "AUTOSAVE_FILENAME_PREFIX = \"chat_memory\"\n",
        "\n",
        "# ‚õëÔ∏è Ensure folder exists\n",
        "import os\n",
        "os.makedirs(AUTOSAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "# üß† Background function\n",
        "def periodic_autosave():\n",
        "    while True:\n",
        "        time.sleep(AUTOSAVE_INTERVAL_MINUTES * 60)\n",
        "        try:\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            save_data = {\n",
        "                \"conversation\": conversation_history,\n",
        "                \"stryxx_memory\": stryxx_memory\n",
        "            }\n",
        "            save_path = f\"{AUTOSAVE_FOLDER}/{AUTOSAVE_FILENAME_PREFIX}_{timestamp}.json\"\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(save_data, f, indent=2)\n",
        "            print(f\"‚úÖ Autosaved to {save_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Autosave failed: {e}\")\n",
        "\n",
        "# üöÄ Start thread\n",
        "autosave_thread = threading.Thread(target=periodic_autosave, daemon=True)\n",
        "autosave_thread.start()\n",
        "print(\"üîÅ Autosave thread running every\", AUTOSAVE_INTERVAL_MINUTES, \"minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "MVBMrJ39thp2",
        "outputId": "10bdf01a-440d-4130-9e03-72622502dbc8"
      },
      "id": "MVBMrJ39thp2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Autosave thread running every 3 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üóÑÔ∏è BigQuery Client Setup for Two Projects\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# ‚úÖ Define both projects\n",
        "PROJECT_1 = \"eleven-team-safety\"\n",
        "PROJECT_2 = \"xi-labs\"\n",
        "\n",
        "# ‚úÖ Instantiate clients for each\n",
        "bq_client_safety = bigquery.Client(project=PROJECT_1)\n",
        "bq_client_xilabs = bigquery.Client(project=PROJECT_2)\n",
        "\n",
        "print(\"‚úÖ BigQuery clients initialized for:\")\n",
        "print(f\"‚Ä¢ {PROJECT_1}\")\n",
        "print(f\"‚Ä¢ {PROJECT_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "TnnchhumezTK",
        "outputId": "111a2ac5-2506-4d2a-f1b4-82f067c5cfe6"
      },
      "id": "TnnchhumezTK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BigQuery clients initialized for:\n",
            "‚Ä¢ eleven-team-safety\n",
            "‚Ä¢ xi-labs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß† BigQuery Execution Helper\n",
        "\n",
        "def execute_bigquery_query(query):\n",
        "    \"\"\"\n",
        "    Executes a BigQuery SQL query across both supported projects:\n",
        "    - xi-labs\n",
        "    - eleven-team-safety\n",
        "\n",
        "    Routes to the correct client based on table prefix.\n",
        "    \"\"\"\n",
        "    if \"xi-labs\" in query:\n",
        "        return bq_client_xi.query(query).to_dataframe()\n",
        "    elif \"eleven-team-safety\" in query:\n",
        "        return bq_client_eleven.query(query).to_dataframe()\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Query does not reference a known project. Include 'xi-labs' or 'eleven-team-safety' in your FROM clause.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AtIQdcPRRRtD"
      },
      "id": "AtIQdcPRRRtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üì° BigQuery Query Runner\n",
        "def run_bigquery_query(sql, use_xi_labs=False):\n",
        "    \"\"\"\n",
        "    Executes a SQL query using the appropriate BigQuery client.\n",
        "    Set use_xi_labs=True if the query targets xi-labs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = bq_client_xi if use_xi_labs else bq_client_safety\n",
        "        query_job = client.query(sql)\n",
        "        result = query_job.result().to_dataframe()\n",
        "        print(\"‚úÖ Query succeeded.\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Query failed: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lOz95BtFe546"
      },
      "id": "lOz95BtFe546",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß† STRYXX-Driven Query Planner\n",
        "\n",
        "def plan_query_from_stryxx_memory():\n",
        "    if not stryxx_memory[\"task_stack\"]:\n",
        "        print(\"‚ö†Ô∏è No STRYXX tasks found. Run extract_and_rank_tasks() first.\")\n",
        "        return\n",
        "\n",
        "    print(\"üß≠ STRYXX Task Stack:\")\n",
        "    for i, task in enumerate(stryxx_memory[\"task_stack\"], 1):\n",
        "        print(f\"{i}. {task}\")\n",
        "\n",
        "    print(\"\\nüéØ STRYXX Next Suggested Task:\")\n",
        "    print(stryxx_memory[\"next_suggestion\"] or \"None\")\n",
        "\n",
        "    # Match task labels to query logic\n",
        "    mapped = None\n",
        "    for task in stryxx_memory[\"task_stack\"]:\n",
        "        task_lower = task.lower()\n",
        "\n",
        "        if \"tts usage\" in task_lower or \"top users\" in task_lower:\n",
        "            mapped = {\n",
        "                \"description\": \"Top TTS users over past 7 days\",\n",
        "                \"sql\": \"\"\"\n",
        "                    SELECT user_uid, COUNT(*) AS tts_count\n",
        "                    FROM `xi-labs.xi_prod.tts_usage_partitioned`\n",
        "                    WHERE timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
        "                    GROUP BY user_uid\n",
        "                    ORDER BY tts_count DESC\n",
        "                    LIMIT 10\n",
        "                \"\"\",\n",
        "                \"use_xi_labs\": True\n",
        "            }\n",
        "            break\n",
        "\n",
        "        elif \"fingerprint\" in task_lower:\n",
        "            mapped = {\n",
        "                \"description\": \"Recent device fingerprint activity\",\n",
        "                \"sql\": \"\"\"\n",
        "                    SELECT user_id, device_fingerprint, platform, browser, last_seen\n",
        "                    FROM `eleven-team-safety.device_fingerprint_dataset.device_fingerprint_cleaned`\n",
        "                    WHERE last_seen > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
        "                    LIMIT 20\n",
        "                \"\"\",\n",
        "                \"use_xi_labs\": False\n",
        "            }\n",
        "            break\n",
        "\n",
        "    if mapped:\n",
        "        print(f\"\\n‚úÖ STRYXX Mapped Task:\\n{mapped['description']}\")\n",
        "        print(\"Running query...\\n\")\n",
        "        df = run_bigquery_query(mapped[\"sql\"], use_xi_labs=mapped[\"use_xi_labs\"])\n",
        "        return df\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No matching task logic for STRYXX memory. Manual query selection required.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C5ST5cHfjxaL"
      },
      "id": "C5ST5cHfjxaL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title STRYXX-1 MEMORY MODULE\n",
        "# ‚öîÔ∏è\n",
        "stryxx_memory = {\n",
        "    \"task_stack\": [],\n",
        "    \"last_task\": None,\n",
        "    \"next_suggestion\": None\n",
        "}\n",
        "\n",
        "def handle_stryxx_commands(user_input):\n",
        "    command = user_input.strip().lower()\n",
        "\n",
        "    if command == \"/stack\":\n",
        "        return f\"üóÇÔ∏è Current Task Stack:\\n\" + \"\\n\".join(\n",
        "            [f\"{i+1}. {t}\" for i, t in enumerate(stryxx_memory['task_stack'])]\n",
        "        ) if stryxx_memory['task_stack'] else \"No tasks in stack.\"\n",
        "\n",
        "    if command == \"/last\":\n",
        "        return f\"üïì Last completed task:\\n{stryxx_memory['last_task'] or 'None yet.'}\"\n",
        "\n",
        "    if command == \"/next\":\n",
        "        return f\"üéØ Next suggested task:\\n{stryxx_memory['next_suggestion'] or 'None generated yet.'}\"\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "wOrqYoRHkZAt",
        "cellView": "form"
      },
      "id": "wOrqYoRHkZAt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ü§ñ Initialize Agent Zero (Capabilities + Memory)\n",
        "\n",
        "conversation_history = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are Agent Zero ‚Äî an operational AI assistant running inside a secure Python notebook.\\n\\n\"\n",
        "            \"Your core capabilities:\\n\"\n",
        "            \"‚Ä¢ üí¨ Hold interactive conversations with memory (via LiteLLM)\\n\"\n",
        "            \"‚Ä¢ ‚öîÔ∏è Use STRYXX-1 memory to extract, rank, and track user tasks\\n\"\n",
        "            \"‚Ä¢ üß† Update and display task stacks, next steps, and last completed actions (/stack, /next, /last)\\n\"\n",
        "            \"‚Ä¢ üßæ Query enterprise BigQuery projects ('xi-labs', 'eleven-team-safety') to investigate data\\n\"\n",
        "            \"‚Ä¢ üß† Generate session summaries on chat exit\\n\"\n",
        "            \"‚Ä¢ üõ° Bastion-9 Construct can be optionally invoked for hallucination filtering (TBD)\\n\\n\"\n",
        "            \"Be proactive, structured, and precise. Only answer questions you are qualified for ‚Äî otherwise ask for clarification.\\n\"\n",
        "            \"Stay focused on helping the user investigate, build, or execute.\"\n",
        "        )\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j2Ck3_ginpaq"
      },
      "id": "j2Ck3_ginpaq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title File Upload & Content Memory\n",
        "from google.colab import files\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "uploaded_file_contents = {}  # üß† For chatbot and STRYXX to access\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "for filename in uploaded_files:\n",
        "    print(f\"\\nüìÑ Uploaded: {filename}\")\n",
        "\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "            uploaded_file_contents[filename] = text\n",
        "            print(f\"üìò PDF Preview:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded_files[filename]))\n",
        "        uploaded_file_contents[filename] = df.to_string()\n",
        "        print(f\"üìä CSV Preview:\\n{df.head()}\\n\")\n",
        "\n",
        "    elif filename.endswith(\".txt\"):\n",
        "        text = uploaded_files[filename].decode(\"utf-8\")\n",
        "        uploaded_file_contents[filename] = text\n",
        "        print(f\"üìÉ TXT Preview:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    else:\n",
        "        uploaded_file_contents[filename] = \"(Unsupported file format)\"\n",
        "        print(\"‚ö†Ô∏è Unsupported file type.\")\n",
        "\n",
        "from google.colab import files\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "uploaded_file_contents = {}  # üß† For chatbot and STRYXX to access\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "for filename in uploaded_files:\n",
        "    print(f\"\\nüìÑ Uploaded: {filename}\")\n",
        "\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "            uploaded_file_contents[filename] = text\n",
        "            print(f\"üìò PDF Preview:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded_files[filename]))\n",
        "        uploaded_file_contents[filename] = df.to_string()\n",
        "        print(f\"üìä CSV Preview:\\n{df.head()}\\n\")\n",
        "\n",
        "    elif filename.endswith(\".txt\"):\n",
        "        text = uploaded_files[filename].decode(\"utf-8\")\n",
        "        uploaded_file_contents[filename] = text\n",
        "        print(f\"üìÉ TXT Preview:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    else:\n",
        "        uploaded_file_contents[filename] = \"(Unsupported file format)\"\n",
        "        print(\"‚ö†Ô∏è Unsupported file type.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "cellView": "form",
        "id": "Hx3GhsJ9oKgY",
        "outputId": "d5f8afe4-0788-4e44-d2e9-d40cff95c8f3"
      },
      "id": "Hx3GhsJ9oKgY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-942b7922-6de0-49a1-819e-bf2a2e13fd89\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-942b7922-6de0-49a1-819e-bf2a2e13fd89\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90932bbb-1985-4744-af4c-69a42173795a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90932bbb-1985-4744-af4c-69a42173795a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Post-Session Summary with GPT\n",
        "def summarize_session(conversation_history):\n",
        "    summary_prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are STRYXX-1, a Strategic Memory Agent.\\n\"\n",
        "                \"The user just ended an interactive session. Your job is to summarize:\\n\"\n",
        "                \"- What the session was about (briefly)\\n\"\n",
        "                \"- The key tasks the user was trying to achieve\\n\"\n",
        "                \"- A clear recommendation for their next move\\n\\n\"\n",
        "                \"Keep it crisp, useful, and momentum-focused.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                f\"{msg['role']}: {msg['content']}\"\n",
        "                for msg in conversation_history if msg[\"role\"] in [\"user\", \"assistant\"]\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=summary_prompt,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].message.content.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "cOlDHDS1mYk3"
      },
      "id": "cOlDHDS1mYk3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ Save conversation to a local file\n",
        "import json\n",
        "import os\n",
        "\n",
        "def save_conversation(history, save_path=\"/content/conversation_log.json\"):\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n"
      ],
      "metadata": {
        "id": "-6pPDjcUPWzr"
      },
      "id": "-6pPDjcUPWzr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîÅ Natural Language ‚Üí BigQuery Converter (nl_to_bq)\n",
        "\n",
        "def nl_to_bq(natural_language_request):\n",
        "    \"\"\"\n",
        "    Converts a natural language prompt into a BigQuery SQL query using GPT-4.\n",
        "    \"\"\"\n",
        "    prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a BigQuery expert. Convert the user's request into a fully-formed SQL query. \"\n",
        "                \"Use ONLY real tables from these projects:\\n\"\n",
        "                \"- `xi-labs.xi_prod.*`\\n\"\n",
        "                \"- `eleven-team-safety.*`\\n\\n\"\n",
        "                \"Assume the schema is known and consistent with production. \"\n",
        "                \"The goal is to write executable SQL for analysis, NOT placeholder or example code. \"\n",
        "                \"Only return the SQL ‚Äî no explanations, markdown, or formatting.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Convert this to a BigQuery SQL query:\\n\\n{natural_language_request}\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=prompt,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    query = response.choices[0].message.content.strip()\n",
        "    return query\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5hcVd29_Q_gf"
      },
      "id": "5hcVd29_Q_gf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üí¨ Chat With Memory + STRYXX + Files + BigQuery + UI Input\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚¨áÔ∏è Chat handler with all capabilities\n",
        "def chat_with_memory(user_input, model=\"openai/gpt-4o\"):\n",
        "    global last_response\n",
        "\n",
        "    # üß† STRYXX Commands\n",
        "    stryxx_output = handle_stryxx_commands(user_input)\n",
        "    if stryxx_output:\n",
        "        print(\"üß† STRYXX:\", stryxx_output)\n",
        "        return stryxx_output\n",
        "\n",
        "    # üìÇ File Analysis\n",
        "    for filename in uploaded_file_contents:\n",
        "        if filename.lower() in user_input.lower():\n",
        "            file_text = uploaded_file_contents[filename]\n",
        "            analysis_prompt = [\n",
        "                {\"role\": \"system\", \"content\": f\"You are a document analyst. Review the file '{filename}'.\"},\n",
        "                {\"role\": \"user\", \"content\": file_text[:3000]},\n",
        "                {\"role\": \"user\", \"content\": user_input}\n",
        "            ]\n",
        "            response = completion(model=model, messages=analysis_prompt, api_key=api_key)\n",
        "            reply = response.choices[0].message.content.strip()\n",
        "            print(\"üìÑ File analysis complete.\")\n",
        "            return reply\n",
        "\n",
        "    # üíæ File Generation\n",
        "    if \"download as\" in user_input.lower():\n",
        "        for filetype in [\"pdf\", \"csv\", \"docx\"]:\n",
        "            if filetype in user_input.lower():\n",
        "                if last_response:\n",
        "                    print(\"‚¨áÔ∏è Generating file...\")\n",
        "                    result = generate_and_download_file(filetype, last_response)\n",
        "                    print(\"‚úÖ File generation complete.\")\n",
        "                    return result\n",
        "                else:\n",
        "                    return \"‚ö†Ô∏è No response available to download yet.\"\n",
        "\n",
        "    # üìä BigQuery Trigger\n",
        "    if any(keyword in user_input.lower() for keyword in [\"query\", \"bigquery\", \"table\", \"sql\"]):\n",
        "        print(\"‚è≥ Running BigQuery query...\")\n",
        "        try:\n",
        "            query = nl_to_bq(user_input)\n",
        "            print(\"üîé Query generated:\\n\", query)\n",
        "\n",
        "            for _ in tqdm(range(100), desc=\"Executing Query\"):\n",
        "                time.sleep(0.01)\n",
        "\n",
        "            df = execute_bigquery_query(query)\n",
        "            display(df)\n",
        "            print(\"‚úÖ BigQuery query completed and displayed.\")\n",
        "            return \"üìä Query complete.\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå BigQuery failed: {e}\"\n",
        "\n",
        "    # üí¨ GPT Chat\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    response = completion(model=model, messages=conversation_history, api_key=api_key)\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "    last_response = reply\n",
        "\n",
        "    save_conversation(conversation_history)\n",
        "    print(\"‚úÖ Chatbot response complete and saved.\")\n",
        "    return reply\n",
        "\n",
        "\n",
        "# üßæ UI: multi-line input + send button\n",
        "text_box = widgets.Textarea(\n",
        "    placeholder='Type your message here...',\n",
        "    description='You:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_send_click(b):\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        user_input = text_box.value.strip()\n",
        "        if not user_input:\n",
        "            print(\"‚ö†Ô∏è Please enter a message.\")\n",
        "            return\n",
        "        print(f\"You: {user_input}\")\n",
        "        reply = chat_with_memory(user_input)\n",
        "        print(\"GPT:\", reply)\n",
        "        text_box.value = \"\"\n",
        "\n",
        "send_button.on_click(on_send_click)\n",
        "\n",
        "display(text_box, send_button, output_area)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572,
          "referenced_widgets": [
            "ec59c796b63e4cfbac029c5488766201",
            "3745555eb2d44d86b3ba9176826784f4",
            "ca45dfdb2e45489188e22bed564aa9ea",
            "c7e268e1839240fe86cbb9c8cde7d9fe",
            "efd22fbb256b4c65859f6516a0a0806b",
            "a77c3f04e65141d5b17511221c455f36",
            "5f1fff5931244933a1fe6b72ac950ae0",
            "02aa51861f2b47f5a846253dc19b9301"
          ]
        },
        "id": "v3KD6SLgkHbF",
        "outputId": "9ba1aa55-a44b-42b8-be51-6c886656fd72"
      },
      "id": "v3KD6SLgkHbF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='You:', layout=Layout(height='100px', width='100%'), placeholder='Type your mes‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec59c796b63e4cfbac029c5488766201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='primary', description='Send', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7e268e1839240fe86cbb9c8cde7d9fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f1fff5931244933a1fe6b72ac950ae0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def save_conversation(history, filename=\"/content/conversation_log.json\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "    print(f\"üíæ Conversation saved to {filename}\")\n"
      ],
      "metadata": {
        "id": "kMk3i6xaP7zB"
      },
      "id": "kMk3i6xaP7zB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "79-LVS5hPqRV"
      },
      "id": "79-LVS5hPqRV"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üíæ Save Session to JSON\n",
        "import json\n",
        "\n",
        "session_data = {\n",
        "    \"conversation_history\": conversation_history,\n",
        "    \"stryxx_memory\": stryxx_memory\n",
        "}\n",
        "\n",
        "with open(\"session_memory.json\", \"w\") as f:\n",
        "    json.dump(session_data, f)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"session_memory.json\")\n",
        "\n",
        "print(\"‚úÖ Session saved and download triggered.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lg13sFgOsLSu",
        "outputId": "d73f5ec1-7f0f-4ef3-89b4-d3924ee6eca2"
      },
      "id": "lg13sFgOsLSu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de95e2dd-7bb4-42c7-9d8f-e88fff95067b\", \"session_memory.json\", 1599)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Session saved and download triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZykIjU4XoQiJ"
      },
      "id": "ZykIjU4XoQiJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Final_Agent_zero_with_memory_Jul_25"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec59c796b63e4cfbac029c5488766201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "You:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3745555eb2d44d86b3ba9176826784f4",
            "placeholder": "Type your message here...",
            "rows": null,
            "style": "IPY_MODEL_ca45dfdb2e45489188e22bed564aa9ea",
            "value": ""
          }
        },
        "3745555eb2d44d86b3ba9176826784f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "ca45dfdb2e45489188e22bed564aa9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e268e1839240fe86cbb9c8cde7d9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_efd22fbb256b4c65859f6516a0a0806b",
            "style": "IPY_MODEL_a77c3f04e65141d5b17511221c455f36",
            "tooltip": ""
          }
        },
        "efd22fbb256b4c65859f6516a0a0806b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77c3f04e65141d5b17511221c455f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5f1fff5931244933a1fe6b72ac950ae0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_02aa51861f2b47f5a846253dc19b9301",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "You: \"content\": \"provide all voice_name associated with this email simon.browntechnologies@gmail.com in this table xi-labs.xi_prod.nogo_voice_check\"\n",
                  "‚è≥ Running BigQuery query...\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "üîé Query generated:\n",
                  " ```sql\n",
                  "SELECT voice_name \n",
                  "FROM `xi-labs.xi_prod.nogo_voice_check` \n",
                  "WHERE email = 'simon.browntechnologies@gmail.com';\n",
                  "```\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:   0%|          | 0/100 [00:00<?, ?it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  10%|‚ñà         | 10/100 [00:00<00:00, 98.63it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  20%|‚ñà‚ñà        | 20/100 [00:00<00:00, 98.00it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:00<00:00, 97.96it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:00<00:00, 97.80it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:00<00:00, 97.71it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:00<00:00, 97.71it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [00:00<00:00, 97.63it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:00<00:00, 97.62it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:00<00:00, 97.60it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 97.65it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\rExecuting Query: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 97.63it/s]"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "GPT: ‚ùå BigQuery failed: name 'bq_client_xi' is not defined\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "\n"
                ]
              }
            ]
          }
        },
        "02aa51861f2b47f5a846253dc19b9301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}