{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "stV4pFdYlqvi"
   },
   "source": [
    "üß† What This Notebook Does\n",
    "ü™û Mirror System Operational Chatbot\n",
    "You‚Äôre running an interactive, agentic GPT-4o chatbot with memory, task awareness, and investigative backend logic ‚Äî right inside Colab.\n",
    "\n",
    "‚úÖ Core Capabilities\n",
    "1. Interactive GPT-4o Chat\n",
    "You can chat in natural language with GPT-4o.\n",
    "\n",
    "The assistant remembers conversation history (via conversation_history).\n",
    "\n",
    "Chat ends cleanly when you type exit, quit, or q.\n",
    "\n",
    "2. STRYXX-1 Construct: Agentic Task Memory\n",
    "Tracks implicit tasks based on your conversation (extract_and_rank_tasks()).\n",
    "\n",
    "Commands you can use:\n",
    "\n",
    "/stack ‚Üí shows your active task stack.\n",
    "\n",
    "/last ‚Üí shows your last completed task.\n",
    "\n",
    "/next ‚Üí shows GPT‚Äôs suggestion for your next best action.\n",
    "\n",
    "You can also trigger STRYXX in manual override mode for things like:\n",
    "\n",
    "\"STRYXX-1: priority map\" ‚Üí prints current mission ranking.\n",
    "\n",
    "\"STRYXX-1: task audit\" ‚Üí checks redundancy.\n",
    "\n",
    "These live in memory as:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "stryxx_memory = {\n",
    "    \"task_stack\": [...],\n",
    "    \"last_task\": \"...\",\n",
    "    \"next_suggestion\": \"...\"\n",
    "}\n",
    "3. BASTION-9 Construct (optional filter)\n",
    "(If enabled later) locks responses to factual zones and filters hallucinations.\n",
    "\n",
    "Currently optional ‚Äî logic can be added for:\n",
    "\n",
    "sandbox filtering\n",
    "\n",
    "active enforcement\n",
    "\n",
    "hallucination detection\n",
    "\n",
    "4. BigQuery Access\n",
    "Two clients are set up:\n",
    "\n",
    "eleven-team-safety\n",
    "\n",
    "xi-labs\n",
    "\n",
    "You can:\n",
    "\n",
    "Query real production datasets directly.\n",
    "\n",
    "Wire natural language to SQL via a future STRYXX extension.\n",
    "\n",
    "Investigate users, fingerprints, voice use, etc.\n",
    "\n",
    "üõ†Ô∏è Advanced Extensibility (Already Wired or Stubbed)\n",
    "Feature\tStatus\tNotes\n",
    "üîó OpenAI API\t‚úÖ\tUsing GPT-4o via LiteLLM\n",
    "üß† Conversation memory\t‚úÖ\tStored in conversation_history\n",
    "‚öî STRYXX task memory\t‚úÖ\tTask stack + suggestions\n",
    "üõ° BASTION-9 logic\tüü°\tFramework uploaded, not enforced yet\n",
    "üìä BigQuery dual-project support\t‚úÖ\televen-team-safety + xi-labs\n",
    "üí¨ STRYXX override commands\t‚úÖ\tVia keyword triggers\n",
    "üß© Notebook modular structure\t‚úÖ\tClean cells by purpose\n",
    "üóÇÔ∏è Summon Cards + Docs\t‚úÖ\tAll loaded + mapped\n",
    "\n",
    "üó£Ô∏è What You Can Ask Right Now\n",
    "You can use this chatbot for:\n",
    "\n",
    "ü§ñ Asking about anything GPT-4o supports (legal help, writing, debugging)\n",
    "\n",
    "‚öî Strategy planning ‚Äî STRYXX will extract and stack your goals\n",
    "\n",
    "üíª Investigations ‚Äî ask things like:\n",
    "\n",
    "\"Query top TTS users by fingerprint\"\n",
    "\n",
    "\"Check device types in xi-labs\"\n",
    "\n",
    "üß† Personal flow tracking ‚Äî STRYXX helps keep momentum\n",
    "\n",
    "üìä Future plans ‚Äî wire your GPT directly into abuse detection, TTS logs, and workspace analysis\n",
    "\n",
    "üí° Next You Could Add:\n",
    "BASTION-9 hallucination filtering per reply (auto or manual)\n",
    "\n",
    "A /query STRYXX command that auto-translates questions to SQL\n",
    "\n",
    "Memory persistence (save/load STRYXX + chat history)\n",
    "\n",
    "Slack or web UI endpoint for team use\n",
    "\n",
    "GPT-based summary at end of chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "0DZPKgSKrXD-"
   },
   "outputs": [],
   "source": [
    "last_response = None  # Used to store last GPT reply for download purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "aptmnJM8yf0MkmiTgYLMRw51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Environment Setup (Agentic GPT + BigQuery Dual Project Support)\n",
    "\n",
    "# Install core libraries\n",
    "!pip install --quiet --upgrade \\\n",
    "    litellm \\\n",
    "    openai \\\n",
    "    tiktoken \\\n",
    "    google-cloud-bigquery \\\n",
    "    google-auth\n",
    "!pip install PyPDF2 --quiet\n",
    "#  Runtime Restart Required (automatically triggered below)\n",
    "import os\n",
    "os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "E7BsaP82qvuJ"
   },
   "outputs": [],
   "source": [
    "!pip install fpdf python-docx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leREZp_qehTV",
    "outputId": "4d5b6934-26b9-4663-e957-e4a4781c82da"
   },
   "outputs": [],
   "source": [
    "# @title üîê OpenAI + LiteLLM Setup\n",
    "from getpass import getpass\n",
    "import openai\n",
    "from litellm import completion\n",
    "\n",
    "# Ask for your API key securely\n",
    "api_key = getpass(\"üîê Enter your OpenAI API key: \")\n",
    "\n",
    "# Set OpenAI key for use with openai lib (if needed)\n",
    "openai.api_key = api_key\n",
    "\n",
    "print(\"‚úÖ API Key stored for OpenAI + LiteLLM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "smQjGWYpsPyo",
    "outputId": "aed1de79-be8b-497b-d792-a5d6edf8ac33"
   },
   "outputs": [],
   "source": [
    "# @title üîÅ Load Session from Uploaded JSON\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "uploaded = files.upload()\n",
    "filename = next(iter(uploaded))  # Take first uploaded file\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "conversation_history = data.get(\"conversation_history\", [])\n",
    "stryxx_memory = data.get(\"stryxx_memory\", {\n",
    "    \"task_stack\": [],\n",
    "    \"last_task\": None,\n",
    "    \"next_suggestion\": None\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Session restored from file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "cellView": "form",
    "id": "X9CKCwoxfOQH"
   },
   "outputs": [],
   "source": [
    "# @title üß† STRYXX + BASTION Memory Initialization\n",
    "\n",
    "# Memory states for constructs\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Stay grounded, remember facts, and guide the user with clarity and focus.\"}\n",
    "]\n",
    "\n",
    "stryxx_memory = {\n",
    "    \"task_stack\": [],\n",
    "    \"last_task\": None,\n",
    "    \"next_suggestion\": None\n",
    "}\n",
    "\n",
    "bastion_memory = {\n",
    "    \"last_truth_check\": None,\n",
    "    \"last_warning\": None\n",
    "}\n",
    "\n",
    "constructs = {\n",
    "    \"STRYXX-1\": {\"active\": False},\n",
    "    \"BASTION-9\": {\"active\": False}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "cellView": "form",
    "id": "_4sj9f-Iet9S"
   },
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Construct Command Parsers (STRYXX + BASTION)\n",
    "\n",
    "def handle_stryxx_commands(user_input):\n",
    "    command = user_input.strip().lower()\n",
    "\n",
    "    if command == \"/stack\":\n",
    "        return \"üóÇÔ∏è Task Stack:\\n\" + \"\\n\".join(\n",
    "            [f\"{i+1}. {t}\" for i, t in enumerate(stryxx_memory['task_stack'])]\n",
    "        ) or \"No tasks in stack.\"\n",
    "\n",
    "    elif command == \"/last\":\n",
    "        return f\"üïì Last completed task:\\n{stryxx_memory['last_task'] or 'None yet.'}\"\n",
    "\n",
    "    elif command == \"/next\":\n",
    "        return f\"üéØ Next suggested task:\\n{stryxx_memory['next_suggestion'] or 'None generated yet.'}\"\n",
    "\n",
    "    return None  # Not a command\n",
    "\n",
    "\n",
    "def handle_bastion_commands(user_input):\n",
    "    if user_input.strip().lower() == \"/truth\":\n",
    "        return f\"üõ°Ô∏è Last truth enforcement:\\n{bastion_memory['last_truth_check'] or 'None yet.'}\"\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "cellView": "form",
    "id": "IrEnUFfKfZx1"
   },
   "outputs": [],
   "source": [
    "# @title üß† STRYXX Extract + Rank Tasks\n",
    "\n",
    "def extract_and_rank_tasks(conversation):\n",
    "    from operator import itemgetter\n",
    "\n",
    "    summary_prompt = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are STRYXX-1, an Execution Strategist.\\n\"\n",
    "            \"Your job is to extract the tasks the user is implicitly or explicitly working on, \"\n",
    "            \"rank them by ROI (return on impact + time), and suggest the next most useful action.\\n\"\n",
    "            \"Only include tasks that drive momentum or clarity. Discard fluff.\\n\\n\"\n",
    "            \"Output:\\n\"\n",
    "            \"- Ranked task stack (list of short task labels)\\n\"\n",
    "            \"- Most recent completed task (if any)\\n\"\n",
    "            \"- Next recommended step\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\"\n",
    "            for msg in conversation if msg[\"role\"] in [\"user\", \"assistant\"]\n",
    "        ])}\n",
    "    ]\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=summary_prompt,\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    text = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        lines = text.splitlines()\n",
    "        task_stack = []\n",
    "        last_task = None\n",
    "        next_suggestion = None\n",
    "        mode = None\n",
    "\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"- Ranked task stack\"):\n",
    "                mode = \"stack\"\n",
    "                continue\n",
    "            elif line.strip().startswith(\"- Most recent completed task\"):\n",
    "                mode = \"last\"\n",
    "                continue\n",
    "            elif line.strip().startswith(\"- Next recommended step\"):\n",
    "                mode = \"next\"\n",
    "                continue\n",
    "\n",
    "            if mode == \"stack\" and line.strip().startswith(tuple(str(i) + \".\" for i in range(1, 10))):\n",
    "                task_stack.append(line.split(\".\", 1)[1].strip())\n",
    "            elif mode == \"last\":\n",
    "                last_task = line.strip(\"- \").strip()\n",
    "            elif mode == \"next\":\n",
    "                next_suggestion = line.strip(\"- \").strip()\n",
    "\n",
    "        # Update memory\n",
    "        stryxx_memory[\"task_stack\"] = task_stack\n",
    "        stryxx_memory[\"last_task\"] = last_task\n",
    "        stryxx_memory[\"next_suggestion\"] = next_suggestion\n",
    "\n",
    "        return \"‚úÖ STRYXX memory updated.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è STRYXX failed to parse response:\\n\\n{text}\\n\\nError: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "qG6F6k8Ufimh"
   },
   "outputs": [],
   "source": [
    "# @title üß† STRYXX Activation + Response Handler\n",
    "def activate_stryxx():\n",
    "    constructs[\"STRYXX-1\"][\"active\"] = True\n",
    "    return (\n",
    "        \"STRYXX-1 online.\\n\"\n",
    "        \"Please confirm desired next action:\\n\"\n",
    "        \"‚Ä¢ full priority map\\n‚Ä¢ task audit of current thread\\n‚Ä¢ or await further instruction\"\n",
    "    )\n",
    "\n",
    "def stryxx_response(user_input, thread=None):\n",
    "    if \"priority map\" in user_input.lower():\n",
    "        return \"**STRYXX-1 PRIORITY MAP**\\n1. Connect chatbot to BigQuery\\n2. Set up memory scaffolding\\n3. Implement user feedback capture\"\n",
    "    elif \"task audit\" in user_input.lower():\n",
    "        return \"**STRYXX-1 TASK AUDIT**\\nNo redundant tasks. Current focus valid.\"\n",
    "    else:\n",
    "        return (\n",
    "            \"STRYXX-1 awaiting operational input. Try commands like:\\n\"\n",
    "            \"‚Ä¢ STRYXX-1: PRIORITY MODE\\n‚Ä¢ STRYXX-1: override emotional pull\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "a3fzHdcbMlkS"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_bigquery_query(query, default_project=\"xi-labs\"):\n",
    "    try:\n",
    "        # üîç Try to detect project from the query itself\n",
    "        match = re.search(r\"FROM\\s+`([\\w\\-]+)\\.\", query, re.IGNORECASE)\n",
    "        project = match.group(1) if match else default_project\n",
    "\n",
    "        client = bigquery.Client(project=project)\n",
    "        result = client.query(query).result().to_dataframe()\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå BigQuery Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "cellView": "form",
    "id": "PVjsMMrnfpHk"
   },
   "outputs": [],
   "source": [
    "# @title Chat With Memory + STRYXX + BigQuery + File Tools\n",
    "# @title üí¨ Chat With Memory + STRYXX + NL BigQuery + File I/O + Auto-Save\n",
    "\n",
    "import time\n",
    "import threading\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "from litellm import completion\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a GPT-4o assistant with full memory, file analysis, and BigQuery access. Always respond concisely and helpfully.\"}]\n",
    "last_response = None\n",
    "bq_progress_bar = None\n",
    "\n",
    "def run_query_with_progress(sql, project=\"xi-labs\"):\n",
    "    global bq_progress_bar\n",
    "\n",
    "    def progress():\n",
    "        for _ in tqdm(range(100), desc=\"üß† Executing BigQuery\", ncols=80):\n",
    "            time.sleep(0.03)\n",
    "\n",
    "    # Start progress bar\n",
    "    thread = threading.Thread(target=progress)\n",
    "    thread.start()\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project=project)\n",
    "    result = client.query(sql).to_dataframe()\n",
    "\n",
    "    thread.join()  # Wait for progress bar to finish\n",
    "    return result\n",
    "\n",
    "def convert_nl_to_sql(user_input):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"Convert the user's request into a BigQuery SQL query. Use only tables from 'xi-labs' and 'eleven-team-safety'. Assume user has access.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    response = completion(model=\"openai/gpt-4o\", messages=prompt, api_key=api_key)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def chat_with_memory(user_input, model=\"openai/gpt-4o\"):\n",
    "    global last_response\n",
    "\n",
    "    # STRYXX command check\n",
    "    stryxx_output = handle_stryxx_commands(user_input)\n",
    "    if stryxx_output:\n",
    "        return f\"üß† STRYXX: {stryxx_output}\"\n",
    "\n",
    "    # üóÇÔ∏è File analysis\n",
    "    for filename in uploaded_file_contents:\n",
    "        if filename.lower() in user_input.lower():\n",
    "            file_text = uploaded_file_contents[filename]\n",
    "            analysis_prompt = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a document analyst. File name: '{filename}'\"},\n",
    "                {\"role\": \"user\", \"content\": file_text[:3000]},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            response = completion(model=model, messages=analysis_prompt, api_key=api_key)\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "    # üì• File download trigger\n",
    "    if \"download as\" in user_input.lower():\n",
    "        for filetype in [\"pdf\", \"csv\", \"docx\"]:\n",
    "            if filetype in user_input.lower():\n",
    "                return generate_and_download_file(filetype, last_response or \"No content to save.\")\n",
    "\n",
    "    # üìä Natural Language ‚Üí SQL ‚Üí Query\n",
    "    if \"query\" in user_input.lower() or \"get\" in user_input.lower():\n",
    "        try:\n",
    "            sql = convert_nl_to_sql(user_input)\n",
    "            # Decide which project\n",
    "            project = \"eleven-team-safety\" if \"eleven\" in sql else \"xi-labs\"\n",
    "            df = run_query_with_progress(sql, project=project)\n",
    "            return df.to_markdown()\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è BigQuery failed:\\n{e}\"\n",
    "\n",
    "    # üí¨ Standard GPT chat\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    response = completion(model=model, messages=conversation_history, api_key=api_key)\n",
    "    reply = response.choices[0].message.content.strip()\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    last_response = reply\n",
    "    return reply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "cellView": "form",
    "id": "iSNWVn4sfvvs"
   },
   "outputs": [],
   "source": [
    "# @title ‚öîÔ∏è STRYXX Activation + Manual Response Handler\n",
    "def activate_stryxx():\n",
    "    constructs[\"STRYXX-1\"][\"active\"] = True\n",
    "    return \"STRYXX-1 online. Please confirm desired next action: full priority map, task audit of current thread, or await further instruction?\"\n",
    "\n",
    "def stryxx_response(user_input, thread):\n",
    "    command = user_input.lower()\n",
    "\n",
    "    if \"priority map\" in command:\n",
    "        return \"**STRYXX-1 PRIORITY MAP**\\n1. Connect chatbot to BigQuery\\n2. Set up memory scaffolding\\n3. Implement user feedback capture\"\n",
    "    elif \"task audit\" in command:\n",
    "        return \"**STRYXX-1 TASK AUDIT**\\nNo redundant tasks. Current focus valid.\"\n",
    "    else:\n",
    "        return \"STRYXX-1 awaiting operational input. Use commands like:\\n‚Ä¢ STRYXX-1: PRIORITY MODE\\n‚Ä¢ STRYXX-1: override emotional pull\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVBMrJ39thp2",
    "outputId": "10bdf01a-440d-4130-9e03-72622502dbc8"
   },
   "outputs": [],
   "source": [
    "# @title üïí Background Autosave (Every 3 minutes)\n",
    "import threading, time, datetime, json\n",
    "\n",
    "# üîß Custom settings\n",
    "AUTOSAVE_INTERVAL_MINUTES = 3\n",
    "AUTOSAVE_FOLDER = \"/content/autosaves\"  # ‚úÖ Change this to any writable path\n",
    "AUTOSAVE_FILENAME_PREFIX = \"chat_memory\"\n",
    "\n",
    "# ‚õëÔ∏è Ensure folder exists\n",
    "import os\n",
    "os.makedirs(AUTOSAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# üß† Background function\n",
    "def periodic_autosave():\n",
    "    while True:\n",
    "        time.sleep(AUTOSAVE_INTERVAL_MINUTES * 60)\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            save_data = {\n",
    "                \"conversation\": conversation_history,\n",
    "                \"stryxx_memory\": stryxx_memory\n",
    "            }\n",
    "            save_path = f\"{AUTOSAVE_FOLDER}/{AUTOSAVE_FILENAME_PREFIX}_{timestamp}.json\"\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(save_data, f, indent=2)\n",
    "            print(f\"‚úÖ Autosaved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Autosave failed: {e}\")\n",
    "\n",
    "# üöÄ Start thread\n",
    "autosave_thread = threading.Thread(target=periodic_autosave, daemon=True)\n",
    "autosave_thread.start()\n",
    "print(\"üîÅ Autosave thread running every\", AUTOSAVE_INTERVAL_MINUTES, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnnchhumezTK",
    "outputId": "111a2ac5-2506-4d2a-f1b4-82f067c5cfe6"
   },
   "outputs": [],
   "source": [
    "# @title üóÑÔ∏è BigQuery Client Setup for Two Projects\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# ‚úÖ Define both projects\n",
    "PROJECT_1 = \"eleven-team-safety\"\n",
    "PROJECT_2 = \"xi-labs\"\n",
    "\n",
    "# ‚úÖ Instantiate clients for each\n",
    "bq_client_safety = bigquery.Client(project=PROJECT_1)\n",
    "bq_client_xilabs = bigquery.Client(project=PROJECT_2)\n",
    "\n",
    "print(\"‚úÖ BigQuery clients initialized for:\")\n",
    "print(f\"‚Ä¢ {PROJECT_1}\")\n",
    "print(f\"‚Ä¢ {PROJECT_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "cellView": "form",
    "id": "AtIQdcPRRRtD"
   },
   "outputs": [],
   "source": [
    "# @title üß† BigQuery Execution Helper\n",
    "\n",
    "def execute_bigquery_query(query):\n",
    "    \"\"\"\n",
    "    Executes a BigQuery SQL query across both supported projects:\n",
    "    - xi-labs\n",
    "    - eleven-team-safety\n",
    "\n",
    "    Routes to the correct client based on table prefix.\n",
    "    \"\"\"\n",
    "    if \"xi-labs\" in query:\n",
    "        return bq_client_xi.query(query).to_dataframe()\n",
    "    elif \"eleven-team-safety\" in query:\n",
    "        return bq_client_eleven.query(query).to_dataframe()\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå Query does not reference a known project. Include 'xi-labs' or 'eleven-team-safety' in your FROM clause.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "cellView": "form",
    "id": "lOz95BtFe546"
   },
   "outputs": [],
   "source": [
    "# @title üì° BigQuery Query Runner\n",
    "def run_bigquery_query(sql, use_xi_labs=False):\n",
    "    \"\"\"\n",
    "    Executes a SQL query using the appropriate BigQuery client.\n",
    "    Set use_xi_labs=True if the query targets xi-labs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bq_client_xi if use_xi_labs else bq_client_safety\n",
    "        query_job = client.query(sql)\n",
    "        result = query_job.result().to_dataframe()\n",
    "        print(\"‚úÖ Query succeeded.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "cellView": "form",
    "id": "C5ST5cHfjxaL"
   },
   "outputs": [],
   "source": [
    "# @title üß† STRYXX-Driven Query Planner\n",
    "\n",
    "def plan_query_from_stryxx_memory():\n",
    "    if not stryxx_memory[\"task_stack\"]:\n",
    "        print(\"‚ö†Ô∏è No STRYXX tasks found. Run extract_and_rank_tasks() first.\")\n",
    "        return\n",
    "\n",
    "    print(\"üß≠ STRYXX Task Stack:\")\n",
    "    for i, task in enumerate(stryxx_memory[\"task_stack\"], 1):\n",
    "        print(f\"{i}. {task}\")\n",
    "\n",
    "    print(\"\\nüéØ STRYXX Next Suggested Task:\")\n",
    "    print(stryxx_memory[\"next_suggestion\"] or \"None\")\n",
    "\n",
    "    # Match task labels to query logic\n",
    "    mapped = None\n",
    "    for task in stryxx_memory[\"task_stack\"]:\n",
    "        task_lower = task.lower()\n",
    "\n",
    "        if \"tts usage\" in task_lower or \"top users\" in task_lower:\n",
    "            mapped = {\n",
    "                \"description\": \"Top TTS users over past 7 days\",\n",
    "                \"sql\": \"\"\"\n",
    "                    SELECT user_uid, COUNT(*) AS tts_count\n",
    "                    FROM `xi-labs.xi_prod.tts_usage_partitioned`\n",
    "                    WHERE timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "                    GROUP BY user_uid\n",
    "                    ORDER BY tts_count DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\",\n",
    "                \"use_xi_labs\": True\n",
    "            }\n",
    "            break\n",
    "\n",
    "        elif \"fingerprint\" in task_lower:\n",
    "            mapped = {\n",
    "                \"description\": \"Recent device fingerprint activity\",\n",
    "                \"sql\": \"\"\"\n",
    "                    SELECT user_id, device_fingerprint, platform, browser, last_seen\n",
    "                    FROM `eleven-team-safety.device_fingerprint_dataset.device_fingerprint_cleaned`\n",
    "                    WHERE last_seen > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "                    LIMIT 20\n",
    "                \"\"\",\n",
    "                \"use_xi_labs\": False\n",
    "            }\n",
    "            break\n",
    "\n",
    "    if mapped:\n",
    "        print(f\"\\n‚úÖ STRYXX Mapped Task:\\n{mapped['description']}\")\n",
    "        print(\"Running query...\\n\")\n",
    "        df = run_bigquery_query(mapped[\"sql\"], use_xi_labs=mapped[\"use_xi_labs\"])\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No matching task logic for STRYXX memory. Manual query selection required.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "cellView": "form",
    "id": "wOrqYoRHkZAt"
   },
   "outputs": [],
   "source": [
    "# @title STRYXX-1 MEMORY MODULE\n",
    "# ‚öîÔ∏è\n",
    "stryxx_memory = {\n",
    "    \"task_stack\": [],\n",
    "    \"last_task\": None,\n",
    "    \"next_suggestion\": None\n",
    "}\n",
    "\n",
    "def handle_stryxx_commands(user_input):\n",
    "    command = user_input.strip().lower()\n",
    "\n",
    "    if command == \"/stack\":\n",
    "        return f\"üóÇÔ∏è Current Task Stack:\\n\" + \"\\n\".join(\n",
    "            [f\"{i+1}. {t}\" for i, t in enumerate(stryxx_memory['task_stack'])]\n",
    "        ) if stryxx_memory['task_stack'] else \"No tasks in stack.\"\n",
    "\n",
    "    if command == \"/last\":\n",
    "        return f\"üïì Last completed task:\\n{stryxx_memory['last_task'] or 'None yet.'}\"\n",
    "\n",
    "    if command == \"/next\":\n",
    "        return f\"üéØ Next suggested task:\\n{stryxx_memory['next_suggestion'] or 'None generated yet.'}\"\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "cellView": "form",
    "id": "j2Ck3_ginpaq"
   },
   "outputs": [],
   "source": [
    "# @title ü§ñ Initialize Agent Zero (Capabilities + Memory)\n",
    "\n",
    "conversation_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are Agent Zero ‚Äî an operational AI assistant running inside a secure Python notebook.\\n\\n\"\n",
    "            \"Your core capabilities:\\n\"\n",
    "            \"‚Ä¢ üí¨ Hold interactive conversations with memory (via LiteLLM)\\n\"\n",
    "            \"‚Ä¢ ‚öîÔ∏è Use STRYXX-1 memory to extract, rank, and track user tasks\\n\"\n",
    "            \"‚Ä¢ üß† Update and display task stacks, next steps, and last completed actions (/stack, /next, /last)\\n\"\n",
    "            \"‚Ä¢ üßæ Query enterprise BigQuery projects ('xi-labs', 'eleven-team-safety') to investigate data\\n\"\n",
    "            \"‚Ä¢ üß† Generate session summaries on chat exit\\n\"\n",
    "            \"‚Ä¢ üõ° Bastion-9 Construct can be optionally invoked for hallucination filtering (TBD)\\n\\n\"\n",
    "            \"Be proactive, structured, and precise. Only answer questions you are qualified for ‚Äî otherwise ask for clarification.\\n\"\n",
    "            \"Stay focused on helping the user investigate, build, or execute.\"\n",
    "        )\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "Hx3GhsJ9oKgY",
    "outputId": "d5f8afe4-0788-4e44-d2e9-d40cff95c8f3"
   },
   "outputs": [],
   "source": [
    "# @title File Upload & Content Memory\n",
    "from google.colab import files\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "uploaded_file_contents = {}  # üß† For chatbot and STRYXX to access\n",
    "\n",
    "uploaded_files = files.upload()\n",
    "\n",
    "for filename in uploaded_files:\n",
    "    print(f\"\\nüìÑ Uploaded: {filename}\")\n",
    "\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "            uploaded_file_contents[filename] = text\n",
    "            print(f\"üìò PDF Preview:\\n{text[:500]}...\\n\")\n",
    "\n",
    "    elif filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(io.BytesIO(uploaded_files[filename]))\n",
    "        uploaded_file_contents[filename] = df.to_string()\n",
    "        print(f\"üìä CSV Preview:\\n{df.head()}\\n\")\n",
    "\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        text = uploaded_files[filename].decode(\"utf-8\")\n",
    "        uploaded_file_contents[filename] = text\n",
    "        print(f\"üìÉ TXT Preview:\\n{text[:500]}...\\n\")\n",
    "\n",
    "    else:\n",
    "        uploaded_file_contents[filename] = \"(Unsupported file format)\"\n",
    "        print(\"‚ö†Ô∏è Unsupported file type.\")\n",
    "\n",
    "from google.colab import files\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "uploaded_file_contents = {}  # üß† For chatbot and STRYXX to access\n",
    "\n",
    "uploaded_files = files.upload()\n",
    "\n",
    "for filename in uploaded_files:\n",
    "    print(f\"\\nüìÑ Uploaded: {filename}\")\n",
    "\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "            uploaded_file_contents[filename] = text\n",
    "            print(f\"üìò PDF Preview:\\n{text[:500]}...\\n\")\n",
    "\n",
    "    elif filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(io.BytesIO(uploaded_files[filename]))\n",
    "        uploaded_file_contents[filename] = df.to_string()\n",
    "        print(f\"üìä CSV Preview:\\n{df.head()}\\n\")\n",
    "\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        text = uploaded_files[filename].decode(\"utf-8\")\n",
    "        uploaded_file_contents[filename] = text\n",
    "        print(f\"üìÉ TXT Preview:\\n{text[:500]}...\\n\")\n",
    "\n",
    "    else:\n",
    "        uploaded_file_contents[filename] = \"(Unsupported file format)\"\n",
    "        print(\"‚ö†Ô∏è Unsupported file type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "cOlDHDS1mYk3"
   },
   "outputs": [],
   "source": [
    "# @title Post-Session Summary with GPT\n",
    "def summarize_session(conversation_history):\n",
    "    summary_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are STRYXX-1, a Strategic Memory Agent.\\n\"\n",
    "                \"The user just ended an interactive session. Your job is to summarize:\\n\"\n",
    "                \"- What the session was about (briefly)\\n\"\n",
    "                \"- The key tasks the user was trying to achieve\\n\"\n",
    "                \"- A clear recommendation for their next move\\n\\n\"\n",
    "                \"Keep it crisp, useful, and momentum-focused.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                f\"{msg['role']}: {msg['content']}\"\n",
    "                for msg in conversation_history if msg[\"role\"] in [\"user\", \"assistant\"]\n",
    "            ])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=summary_prompt,\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "-6pPDjcUPWzr"
   },
   "outputs": [],
   "source": [
    "# üíæ Save conversation to a local file\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_conversation(history, save_path=\"/content/conversation_log.json\"):\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "cellView": "form",
    "id": "5hcVd29_Q_gf"
   },
   "outputs": [],
   "source": [
    "# @title üîÅ Natural Language ‚Üí BigQuery Converter (nl_to_bq)\n",
    "\n",
    "def nl_to_bq(natural_language_request):\n",
    "    \"\"\"\n",
    "    Converts a natural language prompt into a BigQuery SQL query using GPT-4.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a BigQuery expert. Convert the user's request into a fully-formed SQL query. \"\n",
    "                \"Use ONLY real tables from these projects:\\n\"\n",
    "                \"- `xi-labs.xi_prod.*`\\n\"\n",
    "                \"- `eleven-team-safety.*`\\n\\n\"\n",
    "                \"Assume the schema is known and consistent with production. \"\n",
    "                \"The goal is to write executable SQL for analysis, NOT placeholder or example code. \"\n",
    "                \"Only return the SQL ‚Äî no explanations, markdown, or formatting.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Convert this to a BigQuery SQL query:\\n\\n{natural_language_request}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=prompt,\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    query = response.choices[0].message.content.strip()\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572,
     "referenced_widgets": [
      "ec59c796b63e4cfbac029c5488766201",
      "3745555eb2d44d86b3ba9176826784f4",
      "ca45dfdb2e45489188e22bed564aa9ea",
      "c7e268e1839240fe86cbb9c8cde7d9fe",
      "efd22fbb256b4c65859f6516a0a0806b",
      "a77c3f04e65141d5b17511221c455f36",
      "5f1fff5931244933a1fe6b72ac950ae0",
      "02aa51861f2b47f5a846253dc19b9301"
     ]
    },
    "id": "v3KD6SLgkHbF",
    "outputId": "9ba1aa55-a44b-42b8-be51-6c886656fd72"
   },
   "outputs": [],
   "source": [
    "# @title üí¨ Chat With Memory + STRYXX + Files + BigQuery + UI Input\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚¨áÔ∏è Chat handler with all capabilities\n",
    "def chat_with_memory(user_input, model=\"openai/gpt-4o\"):\n",
    "    global last_response\n",
    "\n",
    "    # üß† STRYXX Commands\n",
    "    stryxx_output = handle_stryxx_commands(user_input)\n",
    "    if stryxx_output:\n",
    "        print(\"üß† STRYXX:\", stryxx_output)\n",
    "        return stryxx_output\n",
    "\n",
    "    # üìÇ File Analysis\n",
    "    for filename in uploaded_file_contents:\n",
    "        if filename.lower() in user_input.lower():\n",
    "            file_text = uploaded_file_contents[filename]\n",
    "            analysis_prompt = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a document analyst. Review the file '{filename}'.\"},\n",
    "                {\"role\": \"user\", \"content\": file_text[:3000]},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "            response = completion(model=model, messages=analysis_prompt, api_key=api_key)\n",
    "            reply = response.choices[0].message.content.strip()\n",
    "            print(\"üìÑ File analysis complete.\")\n",
    "            return reply\n",
    "\n",
    "    # üíæ File Generation\n",
    "    if \"download as\" in user_input.lower():\n",
    "        for filetype in [\"pdf\", \"csv\", \"docx\"]:\n",
    "            if filetype in user_input.lower():\n",
    "                if last_response:\n",
    "                    print(\"‚¨áÔ∏è Generating file...\")\n",
    "                    result = generate_and_download_file(filetype, last_response)\n",
    "                    print(\"‚úÖ File generation complete.\")\n",
    "                    return result\n",
    "                else:\n",
    "                    return \"‚ö†Ô∏è No response available to download yet.\"\n",
    "\n",
    "    # üìä BigQuery Trigger\n",
    "    if any(keyword in user_input.lower() for keyword in [\"query\", \"bigquery\", \"table\", \"sql\"]):\n",
    "        print(\"‚è≥ Running BigQuery query...\")\n",
    "        try:\n",
    "            query = nl_to_bq(user_input)\n",
    "            print(\"üîé Query generated:\\n\", query)\n",
    "\n",
    "            for _ in tqdm(range(100), desc=\"Executing Query\"):\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            df = execute_bigquery_query(query)\n",
    "            display(df)\n",
    "            print(\"‚úÖ BigQuery query completed and displayed.\")\n",
    "            return \"üìä Query complete.\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå BigQuery failed: {e}\"\n",
    "\n",
    "    # üí¨ GPT Chat\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    response = completion(model=model, messages=conversation_history, api_key=api_key)\n",
    "    reply = response.choices[0].message.content.strip()\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    last_response = reply\n",
    "\n",
    "    save_conversation(conversation_history)\n",
    "    print(\"‚úÖ Chatbot response complete and saved.\")\n",
    "    return reply\n",
    "\n",
    "\n",
    "# üßæ UI: multi-line input + send button\n",
    "text_box = widgets.Textarea(\n",
    "    placeholder='Type your message here...',\n",
    "    description='You:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_send_click(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        user_input = text_box.value.strip()\n",
    "        if not user_input:\n",
    "            print(\"‚ö†Ô∏è Please enter a message.\")\n",
    "            return\n",
    "        print(f\"You: {user_input}\")\n",
    "        reply = chat_with_memory(user_input)\n",
    "        print(\"GPT:\", reply)\n",
    "        text_box.value = \"\"\n",
    "\n",
    "send_button.on_click(on_send_click)\n",
    "\n",
    "display(text_box, send_button, output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "kMk3i6xaP7zB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_conversation(history, filename=\"/content/conversation_log.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"üíæ Conversation saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "79-LVS5hPqRV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lg13sFgOsLSu",
    "outputId": "d73f5ec1-7f0f-4ef3-89b4-d3924ee6eca2"
   },
   "outputs": [],
   "source": [
    "# @title üíæ Save Session to JSON\n",
    "import json\n",
    "\n",
    "session_data = {\n",
    "    \"conversation_history\": conversation_history,\n",
    "    \"stryxx_memory\": stryxx_memory\n",
    "}\n",
    "\n",
    "with open(\"session_memory.json\", \"w\") as f:\n",
    "    json.dump(session_data, f)\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"session_memory.json\")\n",
    "\n",
    "print(\"‚úÖ Session saved and download triggered.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "ZykIjU4XoQiJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final_Agent_zero_with_memory_Jul_25",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
